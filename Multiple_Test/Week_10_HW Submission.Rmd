
---
title: 'Multiple Testing'
author: "Adam Hendel"
date: "04/10/2018"
output: word_document
fontsize: 12pt
---

Create a Word document from this R Markdown file for the following exercises.  Submit the R markdown file and the knitted Word document via D2L Dropbox.  

## Exercise 1

For this exercise we're going to follow along with the first part of the presentation and compute the error rates for three types of multiple testing correction.  Like the presentation, in this example there are 1000 tests and we know when the null and alternative hypotheses are true.  For each test we have:
    - \large $H_0:$ value is from a normal distribution with $\mu=0$
    - \large $H_a:$ value is from a normal distribution with $\mu>0$

Here is the data and p-values:
```{r}
# Do not change this chunk
set.seed(124)
T0 = 900;
T1 = 100;
x = c( rnorm(T0), rnorm(T1, mean = 3))
P = pnorm(x, lower.tail = FALSE )
```
For the first 900 tests $H_0$ is true while for the last 100 tests $H_a$ is true.  Throughout this problem you should test with $\alpha = 0.05$.

### Part 1a

Using the p-values from above how many discoveries are made?  If the testing were working perfectly how many discoveries should have been made?


### -|-|-|-|-|-|-|-|-|-|-|- Answer 1a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
sum(P < 0.05)
```

There are D=`r sum(P < 0.05)` discoveries, though if it were working perfectly there should have been only `r T1` discoveries.

---

### Part 1b

If no correction (PCER) is made for multiple testing, then compute the Type I error rate, Type II error rate, and False Discovery Rate.  Is the Type I error rate similar to what you would expect?  Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1b -|-|-|-|-|-|-|-|-|-|-|-

```{r}

test  <- P > 0.05
test0 <- test[1:T0]
test1 <- test[(T0+1):(T0+T1)]

# type I error rate
type1.e <- sum(test0==F)/T0
type1.e
# type II error rate
type2.e <- sum(test1==T)/T1
type2.e
# false discovery rate
fd <- sum(test0==F)/ (sum(test0==F) + sum(test1==F))
fd
```

Type I Error Rate: `r round(type1.e, 2)`

Type II Error Rate: `r round(type2.e, 2)`

False Discovery Rate: `r round(fd, 2)`

The Type I error rate is very close to what we'd expect, since we set the significance level to 0.05.

---

### Part 1c

Now attempt to control the family-wise error rate (FWER) using Bonferroni correction.  Compute the Type I error rate, Type II error rate, and False Discovery Rate.  How do these results compare to using no correction as in Part 1b?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1c -|-|-|-|-|-|-|-|-|-|-|-

```{r}

btest  <- p.adjust(P, method='bonf') > 0.05
btest0 <- btest[1:T0]
btest1 <- btest[(T0+1):(T0+T1)]

# type I error
bt1 <- sum(btest0==F)/T0
bt1
# type II error
bt2 <- sum(btest1==T)/T1
bt2
# false discovery rate
bfd <- sum(btest0==F)/(sum(btest0==F) + sum(btest1==F))
bfd
```

Bonf. Type I Error Rate: `r round(bt1, 2)`

Bonf. Type II Error Rate: `r round(bt2, 2)`

Bonf. False Dicovery Rate: `r round(bfd, 2)`

Our Type I error rates are now zero compared to `r round(type1.e, 2)`, which is an improvement. The Type II error rate increased quite a bit (from `r round(type2.e, 2)`). The false discovery rate is now zero compared to `r round(fd, 2)` before.

---

### Part 1d

Repeat Part 1c using the Bonferroni-Holm Step-Down procedure to control FWER.  Are the results any different than when using the ordinary Bonferroni correction?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
holmt  <- p.adjust(P,method='holm') > 0.05
holmt0 <- holmt[1:T0]
holmt1 <- holmt[(T0+1):(T0+T1)]

# type I error rate
sum(holmt0==F)/T0

# type II error rate
sum(holmt1==T)/T1

# false discovery rate 
sum(holmt0==F) / (sum(holmt0==F) + sum(holmt1==F))

```

The Bonferroni-Holm Step-Down procedures compared to standard Bonferroni procedure are exactly the same.

---

### Part 1e

Would either of the Bonferroni correction methods be recommended if you were trying to discover possibly significant results for conducting further research into those results?  Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1e -|-|-|-|-|-|-|-|-|-|-|-

Either method would be recommended. These methods can control the experimentwise (family-wise) error rate rather than the individual error rate. These method evaluate the small number of contrasts that were selected prior to observing the data while preserving the selected experimentwise Type I error rate.

---

### Part 1f

Now apply the Benjamin-Hochberg procedure to achieve a target average False Discovery Rate (FDR) of 5%.  Compute the Type I error rate, Type II error rate, and False Discovery Rate.  Write a brief summary comparing these results to those above.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1f -|-|-|-|-|-|-|-|-|-|-|-

```{r}
fdrt  <- p.adjust(P, method='BH') > 0.05
fdrt0 <- fdrt[1:T0]
fdrt1 <- fdrt[(T0+1):(T0+T1)]

# type I error rate 
sum(fdrt0==F)/T0
# ttype II error rate
sum(fdrt1==T)/T1
# false discovery rate
sum(fdrt0==F) / (sum(fdrt0==F) + sum(fdrt1==F))

```

Type I Error Rate: `r round(sum(fdrt0==F)/T0,2)`

Type II Error Rate: `r round(sum(fdrt1==T)/T1,2)`

False Discovery Rate: `r round(sum(fdrt0==F) / (sum(fdrt0==F) + sum(fdrt1==F)),2)`

We still have a small Type I error rate and there is a much lower Type II error rate than BH. The false discovery rate is almost exactly on the target we set (0.05).

---

## Exercise 2

A pharmaceutical company is doing preliminary hypothesis testing of hundreds of compounds to see which ones might be useful in treating a rare form of cancer.  What form of multiple testing correction should the company use (none, Bonferroni, or FDR)?  Explain your reasoning.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2 -|-|-|-|-|-|-|-|-|-|-|-

With a deadly disease like cancer, it would be more important to error on the side of false negatives, that is Type II errors. It is more conservative to erroneously conclude that a compound does not work, and thus not give it to an afflicted patient than erroneously say a compound works and give it to an afflicted patient. More boldly stated, if we are treat an afflicted patient with a compound that does not work, they could die.

However, since the company needs to first explore hundreds of compounds, they probably do not want to miss out on too many of the discoveries.

Thus, at first they might be interested in FDR to limite Type I errors, then use FWER (Bonferroni) when it comes to making decisions about which compounds to administer to a patient.

---

## Exercise 3

Cars were selected at random from among 1993 passenger car models that were listed in both the Consumer Reports issue and the PACE Buying Guide. Pickup trucks and Sport/Utility vehicles were eliminated due to incomplete information in the Consumer Reports source. Duplicate models (e.g., Dodge Shadow and Plymouth Sundance) were listed at most once.  Use the data set Cars93 to do the following. (Type ?Cars93 to learn more about the data.)

For the next two exercises we are going to use the Cars93 data set from the MASS package.  We'll delete the data having to do with vans so that we are only dealing with cars.  The code to load and prepare the data is here:

```{r echo=FALSE, message=FALSE, warning = FALSE}
# Do not change this chunk of code
if (!require(MASS)){
  install.packages('MASS')
  library(MASS)
}
data(Cars93)
Cars93 <- Cars93[Cars93$Type != 'Van',]
Cars93$Type <- factor(Cars93$Type) # recasting Type forces the factor levels to reset
# shorten level labels to make them fit on boxplots
# Cm = Compact
# Lg = Large
# Md = Midsize
# Sm = Small
# Sp = Sporty
Cars93$Type <- factor(Cars93$Type,labels=c('Cm','Lg','Md','Sm','Sp'))
```

Throughout this exercise we'll compare population mean engine revolutions per minute at maximum horsepower (RPM) of the different types of cars (Type). 

### Part 3a

Make a boxplot of RPM~Type.  Is it reasonable to assume the RPM distributions are normal and have equal variances for the different types of cars?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
boxplot(RPM~Type, data = Cars93)
```

By visual inspection, yes we can assume the RPM distributions are *generally* normal with equal variance (without being too picky).

---

### Part 3b

Conduct pairwise t-tests with no correction for multiple testing to see which mean RPM's are different from each other.  Summarize your findings including a brief description of which types of cars have significanlt different mean RPM ($\alpha = 0.05$).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3b -|-|-|-|-|-|-|-|-|-|-|-

```{r}

b3 <- pairwise.t.test(Cars93$RPM,Cars93$Type, p.adjust.method = 'none', pool.sd=T)$p.value < 0.05
b3

for(row in 1:nrow(b3)){
  rowname <- rownames(b3)[row]
  sig     <- names(which(b3[row,]))
  print(paste(rowname, sig, sep=' and '))
}


```

There are 4 significant differences out of 10, though Type I errors may be present. 

The pairs of cars shown above have significantly different mean RPMs.

---

### Part 3c

Repeat 3b, but this time use Bonferroni correction.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3c -|-|-|-|-|-|-|-|-|-|-|-

```{r}

b4 <- pairwise.t.test(Cars93$RPM,Cars93$Type, p.adjust.method = 'bonf', pool.sd=T)$p.value < 0.05
b4

for(row in 1:nrow(b4)){
  rowname <- rownames(b4)[row]
  sig     <- names(which(b4[row,]))
  print(paste(rowname, sig, sep=' and '))
}
```

There are 4 significant differences out of 10, though Type I errors may be present.

The pairs of cars shown above have significantly different mean RPMs. The results are the same at this significance level with our without Bonferroni correction.

---

### Part 3d

Now suppose we actually need to estimate the differences in the population mean RPM types while controlling for Type I errors using the Bonferroni correction.  Use the onewayComp() function from the DS705data package with adjust = 'bonferroni' to compute the CI's with 95% overall confidence.  How much larger is the mean RPM for small cars than for large cars according to your estimates?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
library(DS705data)

d3 <- onewayComp(RPM~Type,data=Cars93,var.equal=TRUE,
           adjust='bonferroni')$comp[,c(2,3,5,6)]
d3
```

The mean RPM for small cars is between `r round(d3['Sm-Lg','lwr'])` and `r round(d3['Sm-Lg', 'upr'])` greater than larger cars (at 95% confidence).

---

### Part 3e

Repeat 3d, but this time use the Tukey-Kramer procedure (use onewayComp() with adust = 'one.step' and var.equal=TRUE ).  Again, how much larger is the mean RPM for small cars than for large cars according to your estimates?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3e -|-|-|-|-|-|-|-|-|-|-|-

```{r}
e3 <- onewayComp(RPM~Type,data=Cars93,var.equal=TRUE,
           adjust='one.step')$comp[,c(2,3,5,6)]
e3
```

The mean RPM for small cars is between `r round(e3['Sm-Lg','lwr'])` and `r round(e3['Sm-Lg', 'upr'])` greater than larger cars (at 95% confidence).

---


### Part 3f

Simultaneous confidence intervals increase the width of the individual intervals to limit the probability that one or more of the intervals are wrong.  Both Bonferroni and Tukey-Kramer can provide the family of simultaneous confidence intervals and maintain an overall confidence level of 95%.    Compare your results from 3d and 3e.  Which set of intervals do you think is better?  Why?


### -|-|-|-|-|-|-|-|-|-|-|- Answer 3f  -|-|-|-|-|-|-|-|-|-|-|-

```{r, echo=F}
f3 <- data.frame(lwr=c(d3['Sm-Lg','lwr'], e3['Sm-Lg','lwr']),
                 upr=c(d3['Sm-Lg','upr'], e3['Sm-Lg','upr']),
                 row.names = c('Bonf','T-K'))
f3$diff <- f3$upr-f3$lwr
f3
```

We see the results of Bonferroni and Tukey-Kramer above. The results are similar both in terms of the width and location of the interval. Tukey-Kramer provides a tighter interval, and both tails are "inside" the bounds of Bonferroni, so it the preferred choice of the two intervals.

---

### Part 3g

Even when you're using a parametric procedure (one that assumes normality for instance), it can be useful to bootstrap the results to validate the choice of parametric procedure.  Use onewayComp() to get the Tukey-Kramer confidence intervals with 95% confidence, but add nboot=10000 to have the code appoximate the critical values used to construct the intervals using bootstrapping.  How do the results compare the theoretically derived Tukey-Kramer results from 3e?  Does this increase your belief in the validity of the theoretical Tukey-Kramer results?  

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3g -|-|-|-|-|-|-|-|-|-|-|-

```{r}
g3 <- onewayComp(RPM~Type,data=Cars93,var.equal=TRUE,
                 adjust='one.step',nboot = 10000)$comp[,c(2,3,5,6)]
g3

bootresults <- c(round(g3['Sm-Lg','lwr']), round(g3['Sm-Lg','upr']))

```

Bootstrapping the Tukey-Kramer method provides an interval of `r round(g3['Sm-Lg','lwr'])` to `r round(g3['Sm-Lg','upr'])`. The standard Tukey-Kramer method gave results of `r round(e3['Sm-Lg','lwr'])` to `r round(e3['Sm-Lg','upr'])`, so the bootstrap interval is wider, but in the same general location.

---

## Exercise 4

Now we are going to analyze differences in prices for different types of cars in the Cars93 data set.  The boxplot below shows that the prices are skewed and variances are different.   

```{r}
boxplot(Price~Type,horizontal=TRUE,data=Cars93)
```


### Part 4a

It should be fairly clear that the price data is not from  normal distributions, at least for several of the car types, but ignore that for now and use the Games-Howell procedure with confidence level 90% to do simultaneous comparisons (if interpreting the $P$-values use $\alpha=0.1$).  (You can use onewayComp() with var.equal=FALSE and ajust = 'one.step').  Use the CI's to identify and interpret the largest significant difference in population mean prices.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 4a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
library(dplyr)
a4 <- onewayComp(RPM~Type,data=Cars93,var.equal=FALSE,alpha = 0.05,
                 adjust='one.step')$comp[,c(2,3,5,6)] %>% data.frame()

alpha = 0.1
# filter out those now significant at alpha
a4 <- a4[a4$p.adj<alpha,]
a4$diff <- a4$upr - a4$lwr
a4

```

---

Above, we can see that the largest significant difference in population mean prices is that between small and large cars.

### Part 4b

Now repeat 4a, but since the price distributions are skewed use bootstrapping by specifying nboot=10000 in onewayComp().  Summarize how these results are different than in 4a.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 4b -|-|-|-|-|-|-|-|-|-|-|-

```{r}
b4 <- onewayComp(RPM~Type,data=Cars93,var.equal=FALSE,alpha = 0.05, nboot = 10000,
                 adjust='one.step')$comp[,c(2,3,5,6)] %>% data.frame()

alpha = 0.1
# filter out those now significant at alpha
b4 <- b4[b4$p.adj<alpha,]
b4$diff <- b4$upr-b4$lwr
b4

```

---

The results are very similar, though the intervals are wider in the bootstrap method.

### Part 4c

Are the results in 4a and 4b very different?  Which results seem more trustworthy, those in 4a or in 4b? Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 4c -|-|-|-|-|-|-|-|-|-|-|-

The bootstrap method may be more trustworthy since we know that the distributions of price are not normal.

----

### Part 4d

Since the prices are skewed it might be better to report differences in medians than in means.  Use the boot package and Bonferroni correction to bootstrap 4 simultaneous confidence intervals with overall confidence level 90% for the difference in population median price between Sporty Cars and each of the other four car types.  You don't need to interpret the results.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 4d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
require(boot)

# assign indices for each of the car types (they are alphabetic order)
sm <- which(levels(Cars93$Type) == 'Sm')
md <- which(levels(Cars93$Type) == 'Md')
cm <- which(levels(Cars93$Type) == 'Cm')
lg <- which(levels(Cars93$Type) == 'Lg')
sp <- which(levels(Cars93$Type) == 'Sp')

bootMedDiff <- function(d,i){
  meds <- tapply(d[i,1],d[,2],median)
  c( meds[sp]-meds[lg], meds[sp]-meds[cm], meds[sp]-meds[md], meds[sp]-meds[sm])
  
}

boot.object <- boot(Cars93[,c('Price','Type')], bootMedDiff, R = 5000, 
                    strata = Cars93$Type)

# 'Sporty - Large'
#boot.ci(boot.object,conf = 1 - .1/3, type='bca', index=1)$bca[4:5]
# 'Sporty - Compact'
#boot.ci(boot.object,conf = 1 - .1/3, type='bca', index=2)$bca[4:5]
# 'Sporty - Medium'
#boot.ci(boot.object,conf = 1 - .1/3, type='bca', index=3)$bca[4:5]
# 'Sporty - Small'
#boot.ci(boot.object,conf = 1 - .1/3, type='bca', index=4)$bca[4:5]

labels <- c('Sporty - Large', 'Sporty - Compact','Sporty - Medium', 'Sporty - Small')
for(i in 1:length(labels)){
  print(labels[i])
  print(boot.ci(boot.object,conf = 1 - .1/3, type='bca', index=i)$bca[4:5] %>% round(2))
  print('----')
}

```

---
